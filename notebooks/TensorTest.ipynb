{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a59d0c28-70a7-402a-bfa4-0550c3b5bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fbe68d-1752-40be-b271-11634cea4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace 'path_to_your_dataset.csv' with the actual path to your dataset\n",
    "dataset_path = '../data/raw/Mental_health_dataset.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "330ae449-c189-4841-8a9e-1c2eddb91fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Mental Health Indicator</th>\n",
       "      <th>Source Platform</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23-08-22 09:22</td>\n",
       "      <td>Feeling disconnected from everyone around me.</td>\n",
       "      <td>negative</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23-01-10 03:05</td>\n",
       "      <td>Struggling to find motivation for even the sma...</td>\n",
       "      <td>negative</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23-04-11 09:15</td>\n",
       "      <td>Feeling quite overwhelmed by everything. Need ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23-03-23 11:19</td>\n",
       "      <td>Had a great workout today! Feeling energized a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>well-being</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23-09-26 19:29</td>\n",
       "      <td>Struggling to find motivation for even the sma...</td>\n",
       "      <td>negative</td>\n",
       "      <td>loneliness</td>\n",
       "      <td>1</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Timestamp                                            Content  \\\n",
       "0  23-08-22 09:22      Feeling disconnected from everyone around me.   \n",
       "1  23-01-10 03:05  Struggling to find motivation for even the sma...   \n",
       "2  23-04-11 09:15  Feeling quite overwhelmed by everything. Need ...   \n",
       "3  23-03-23 11:19  Had a great workout today! Feeling energized a...   \n",
       "4  23-09-26 19:29  Struggling to find motivation for even the sma...   \n",
       "\n",
       "  Sentiment      Topics  Mental Health Indicator Source Platform Language  \n",
       "0  negative     anxiety                        1        Facebook  English  \n",
       "1  negative     anxiety                        1       Instagram  English  \n",
       "2  negative     anxiety                        1       Instagram  English  \n",
       "3  positive  well-being                        0          Reddit  English  \n",
       "4  negative  loneliness                        1       Instagram  English  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c57721-3a29-4cf8-a3b9-1101d8bb6742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove URLs, special characters, and numbers\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\W+|\\d+', ' ', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords and apply lemmatization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function to the Content column\n",
    "df['Content'] = df['Content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91350b1c-e1f9-4dad-907e-7bef092bd0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Mental Health Indicator</th>\n",
       "      <th>Source Platform</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23-08-22 09:22</td>\n",
       "      <td>feeling disconnected everyone around</td>\n",
       "      <td>negative</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23-01-10 03:05</td>\n",
       "      <td>struggling find motivation even smallest task</td>\n",
       "      <td>negative</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23-04-11 09:15</td>\n",
       "      <td>feeling quite overwhelmed everything need talk...</td>\n",
       "      <td>negative</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>1</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23-03-23 11:19</td>\n",
       "      <td>great workout today feeling energized positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>well-being</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23-09-26 19:29</td>\n",
       "      <td>struggling find motivation even smallest task</td>\n",
       "      <td>negative</td>\n",
       "      <td>loneliness</td>\n",
       "      <td>1</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Timestamp                                            Content  \\\n",
       "0  23-08-22 09:22               feeling disconnected everyone around   \n",
       "1  23-01-10 03:05      struggling find motivation even smallest task   \n",
       "2  23-04-11 09:15  feeling quite overwhelmed everything need talk...   \n",
       "3  23-03-23 11:19     great workout today feeling energized positive   \n",
       "4  23-09-26 19:29      struggling find motivation even smallest task   \n",
       "\n",
       "  Sentiment      Topics  Mental Health Indicator Source Platform Language  \n",
       "0  negative     anxiety                        1        Facebook  English  \n",
       "1  negative     anxiety                        1       Instagram  English  \n",
       "2  negative     anxiety                        1       Instagram  English  \n",
       "3  positive  well-being                        0          Reddit  English  \n",
       "4  negative  loneliness                        1       Instagram  English  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0712247-b934-4c16-b70c-79a5a13cb993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Clean and preprocess text as before\n",
    "# Assume 'df' is already loaded and 'Content' column is cleaned\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df['Content'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Content'])\n",
    "X_padded = pad_sequences(sequences, maxlen=100)  # Assume maxlen=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154d3279-14df-4706-a6a2-259cc9912579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Sentiment and Topics as one-hot encoded arrays\n",
    "sentiment_encoder = LabelEncoder()\n",
    "topics_encoder = LabelEncoder()\n",
    "\n",
    "y_sentiment = sentiment_encoder.fit_transform(df['Sentiment'])\n",
    "y_topics = topics_encoder.fit_transform(df['Topics'])\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "y_sentiment = onehot_encoder.fit_transform(y_sentiment.reshape(-1, 1)).toarray()\n",
    "y_topics = onehot_encoder.fit_transform(y_topics.reshape(-1, 1)).toarray()\n",
    "\n",
    "\n",
    "# Mental Health Indicator as a binary label\n",
    "y_mh_indicator = np.array(df['Mental Health Indicator'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5c3c673-5f65-445f-8ff7-56e6b748b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(100,))  # 100 is the maxlen of padded sequences\n",
    "\n",
    "# Shared layers\n",
    "embedding_layer = Embedding(input_dim=10000, output_dim=128)(input_layer)\n",
    "lstm_layer = LSTM(64)(embedding_layer)\n",
    "\n",
    "# Output layers\n",
    "sentiment_output = Dense(y_sentiment.shape[1], activation='softmax', name='sentiment')(lstm_layer)\n",
    "topics_output = Dense(y_topics.shape[1], activation='softmax', name='topics')(lstm_layer)\n",
    "mh_indicator_output = Dense(1, activation='sigmoid', name='mh_indicator')(lstm_layer)\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs=input_layer, outputs=[sentiment_output, topics_output, mh_indicator_output])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'sentiment': 'categorical_crossentropy',\n",
    "                    'topics': 'categorical_crossentropy',\n",
    "                    'mh_indicator': 'binary_crossentropy'},\n",
    "              metrics={'sentiment': ['accuracy'],\n",
    "                       'topics': ['accuracy'],\n",
    "                       'mh_indicator': ['accuracy']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def74deb-440c-440b-ba3c-596869fd4148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 49ms/step - loss: 1.3159 - mh_indicator_accuracy: 0.9673 - sentiment_accuracy: 0.9707 - topics_accuracy: 0.3260 - val_loss: 1.1019 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3387\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 51ms/step - loss: 1.1058 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3266 - val_loss: 1.1022 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3335\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 49ms/step - loss: 1.1033 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3390 - val_loss: 1.1021 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3367\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 49ms/step - loss: 1.1035 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3300 - val_loss: 1.1030 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3340\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 49ms/step - loss: 1.1025 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3337 - val_loss: 1.0993 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3335\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 50ms/step - loss: 1.1028 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3302 - val_loss: 1.1027 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3357\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 49ms/step - loss: 1.1017 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3319 - val_loss: 1.1000 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3289\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 50ms/step - loss: 1.1011 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3421 - val_loss: 1.1023 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3289\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 50ms/step - loss: 1.1018 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3328 - val_loss: 1.1004 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3387\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 51ms/step - loss: 1.1017 - mh_indicator_accuracy: 1.0000 - sentiment_accuracy: 1.0000 - topics_accuracy: 0.3321 - val_loss: 1.1011 - val_mh_indicator_accuracy: 1.0000 - val_sentiment_accuracy: 1.0000 - val_topics_accuracy: 0.3396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2316b7825c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_padded, {'sentiment': y_sentiment, 'topics': y_topics, 'mh_indicator': y_mh_indicator},\n",
    "          batch_size=32, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b5c3baf-f1fe-48a8-b80b-16ba4624c1e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assume X_test, y_test_sentiment, y_test_topics, and y_test_mh_indicator are prepared\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mX_test\u001b[49m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m: y_test_sentiment, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics\u001b[39m\u001b[38;5;124m'\u001b[39m: y_test_topics, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmh_indicator\u001b[39m\u001b[38;5;124m'\u001b[39m: y_test_mh_indicator})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume X_test, y_test_sentiment, y_test_topics, and y_test_mh_indicator are prepared\n",
    "model.evaluate(X_test, {'sentiment': y_test_sentiment, 'topics': y_test_topics, 'mh_indicator': y_test_mh_indicator})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b881bc75-6290-4c9a-8d97-1890ddfc644e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "(array(['positive'], dtype=object), array(['happiness'], dtype=object), array([[0]]))\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "(array(['negative'], dtype=object), array(['depression'], dtype=object), array([[1]]))\n"
     ]
    }
   ],
   "source": [
    "def predict_new_input(model, tokenizer, new_input):\n",
    "    # Tokenize and pad the new input\n",
    "    seq = tokenizer.texts_to_sequences([new_input])\n",
    "    padded = pad_sequences(seq, maxlen=100)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(padded)\n",
    "    sentiment_pred, topics_pred, mh_indicator_pred = prediction\n",
    "    \n",
    "    # Decode predictions\n",
    "    sentiment_label = sentiment_encoder.inverse_transform([np.argmax(sentiment_pred)])\n",
    "    topics_label = topics_encoder.inverse_transform([np.argmax(topics_pred)])\n",
    "    mh_indicator_label = (mh_indicator_pred > 0.5).astype(int)\n",
    "    \n",
    "    return sentiment_label, topics_label, mh_indicator_label\n",
    "\n",
    "# Example usage\n",
    "# new_input = \"I'm feeling quite happy today, but a bit anxious about tomorrow.\"\n",
    "custom_texts = [\n",
    "    \"Just finished a great book on ancient history and I'm feeling inspired!\",\n",
    "    \"Lately, I've been feeling overwhelmed with worry about things that are out of my control.\",\n",
    "    \"Starting meditation has significantly improved my overall sense of well-being.\",\n",
    "    \"The weather has been quite unpredictable this week, with rain and sunshine alternating.\",\n",
    "    \"No matter what I do, there's a persistent feeling of sadness that I can't seem to shake off.\",\n",
    "    \"After months of therapy, I'm finally starting to see improvements in how I feel about myself and my life.\",\n",
    "    \"Recent studies suggest that spending time in nature can have a positive effect on mental health.\",\n",
    "    \"Deadlines are approaching fast, and I'm starting to doubt if I can handle the pressure.\",\n",
    "    \"I am so grateful for the support group I've found; it's comforting to know I'm not alone in this journey.\",\n",
    "    \"I'm hopeful about the future but anxious about the changes it might bring to my personal life and mental health.\"\n",
    "]\n",
    "for new_input in custom_texts:\n",
    "    print(predict_new_input(model, tokenizer, new_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf155a36-69c7-4be0-b658-67a09c22a337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
